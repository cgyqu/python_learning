{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\MyDemo\\python_learning\\testEnv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2f808e15abc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\MyDemo\\python_learning\\testEnv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "D:\\MyDemo\\python_learning\\ml\n"
     ]
    }
   ],
   "source": [
    "APP_DIR = os.path.dirname(os.path.realpath('__file__'))\n",
    "base_dir = os.path.dirname(APP_DIR)\n",
    "windows_size = 50\n",
    "n_step_out = 3\n",
    "model_path = f'{base_dir}/data/hotel_call_predict.h5'\n",
    "scaler_path = f'{base_dir}/data/hotel_call_scaler.npy'\n",
    "normalize_param_path = f'{base_dir}/data/hotel_call_normalize.txt'\n",
    "print(APP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#def parse(x):\n",
    "#   arr = x.split(' ')\n",
    "#   hour = str(int(int(arr[-1])/60))\n",
    "#   min = str(int(arr[-1])%60)\n",
    "#   arr[-1] = hour\n",
    "#   arr.append(min)\n",
    "#   time = ' '.join(arr)\n",
    "#   return datetime.datetime.strptime(time, '%Y %m %d %H %M')\n",
    "#path = f'{base_dir}//data//hotel_call_test.csv'\n",
    "#df = pd.read_csv(path, parse_dates = [['Date', 'Month','Day','Time']], index_col=0, date_parser=parse,keep_date_col=True)\n",
    "#df.index.name = 'date_time'\n",
    "#df = df.sort_values('date_time')\n",
    "#df['IsPeak'] = df['Time'].apply(lambda x : 1 if int(x) in range(11*60,60*14,30) else 0)\n",
    "#df = df.loc[:, ['Time','ProductRate','AHT','DayOfWeek','IsPeak', 'TotalCount']]\n",
    "#df.to_csv(f'{base_dir}//data//hotel_call_datetime_test.csv')\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data trend\n",
    "def getPlotData():\n",
    "    path = f'{base_dir}//data//hotel_call_datetime.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for row_index, row in df.iterrows():\n",
    "        if(row_index > 48*7):\n",
    "            break\n",
    "        try:\n",
    "            x = str(row[0])\n",
    "            y = row[-1]\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        except Exception as ex:\n",
    "            print(\"error row ,index:\" , row_index)\n",
    "    return X,Y \n",
    "x,y = getPlotData()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df,rows = []):\n",
    "   param = 'row,mean,std\\n'\n",
    "   for row in rows:\n",
    "        arr =  np.array(df[row])\n",
    "        mean = arr.mean()\n",
    "        std = arr.std()\n",
    "        res = list()\n",
    "        for i in range(len(arr)):\n",
    "            res.append((arr[i] -mean)/std)\n",
    "        df[row] = res\n",
    "        param +=f'{row},{mean},{std}\\n'\n",
    "   with open(normalize_param_path,'w') as f:\n",
    "        f.writelines(param)\n",
    "\n",
    "def inverse_data(row_name,data=[]):\n",
    "    if(len(row_name) <=0):\n",
    "        return data\n",
    "    df = pd.read_csv(normalize_param_path)\n",
    "    row = np.array(df[df.row == row_name])\n",
    "    if(len(row) <= 0):\n",
    "        return data\n",
    "    row = row[0]\n",
    "    mean = row[1]\n",
    "    std = row[2]\n",
    "    res = list()\n",
    "    for d in data:\n",
    "        res.append(d*std + mean)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filepath):\n",
    "    frame = []\n",
    "    for file in filepath:\n",
    "        path = f'{base_dir}//data//{file}'\n",
    "        frame.append(pd.read_csv(path,index_col=0))\n",
    "    df = pd.concat(frame)\n",
    "    df_copy =df.copy()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(df)\n",
    "    # 保存归一化参数\n",
    "    np.save(scaler_path,scaler)\n",
    "    #normalize_data(df,[\"TotalCount\",'Time','AHT'])\n",
    "    #scaled = np.array(df)\n",
    "    data_size = len(scaled)\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for i in range(data_size - windows_size - n_step_out):\n",
    "        end = i + windows_size\n",
    "        X.append(scaled[i:end, :])\n",
    "        Y.append(scaled[end:end + n_step_out,-1].reshape(3))\n",
    "    return np.array(X), np.array(Y),df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prepare data...\")\n",
    "X,Y,df = loadData([\"hotel_call_datetime.csv\",\"hotel_call_datetime_test.csv\"])\n",
    "#test_x, test_y,df = loadData([\"hotel_call_datetime_test.csv\"])\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "train_size = int(len(X)*0.9)\n",
    "train_x = X[0:train_size,:,:]\n",
    "train_y = Y[0:train_size,:]\n",
    "test_x = X[train_size:-1,:,:]\n",
    "test_y = Y[train_size:-1,:]\n",
    "#train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print(train_x.shape,test_x.shape)\n",
    "print(train_y.shape,test_y.shape)\n",
    "print(\"prepare data complete.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit1 = 150\n",
    "unit2 = 150\n",
    "epochs=25 # 20次往后的收缩就很慢了，这里到25\n",
    "batch_size=32 * 2**2 # 测试128最佳\n",
    "hidden_layer = 0\n",
    "def getModel():\n",
    "    print(\"create model...\")\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(unit1, activation='relu', return_sequences=True,input_shape=(windows_size, X.shape[2])))\n",
    "    #for i in range(hidden_layer):\n",
    "    #     model.add(LSTM(unit1, activation='relu', return_sequences=True))\n",
    "    # 最后一层\n",
    "    model.add(LSTM(unit2, activation='relu'))\n",
    "    model.add(Dense(n_step_out))\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer= adam, loss='mse')\n",
    "    print(\"create model complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"strat training...\")\n",
    "time_start = time.time()\n",
    "model = getModel()\n",
    "print(\"train_x.shape\",train_x.shape,\"train_y.shape\",train_y.shape)\n",
    "print(\"test_x.shape\",test_x.shape,\"test_y.shape\",test_y.shape)\n",
    "print(\"windows_size:\",windows_size)\n",
    "print(\"n_features\",X.shape[2])\n",
    "print(f'epochs:{epochs},batch_size:{batch_size}')\n",
    "# fit model\n",
    "history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,validation_data=(test_x, test_y))\n",
    "time_end = time.time()\n",
    "print(\"save model，path:\" ,model_path)\n",
    "model.save(model_path)\n",
    "print(\"train success,cost:\", time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.array(history.history['loss']).min()\n",
    "with open(f'{base_dir}/data/train_history.txt','a') as f:\n",
    "    f.write(f'unit1:{unit1},unit2:{unit2},epochs:{epochs},batch_size:{batch_size},hidden_layer:{hidden_layer},min_loss:{min_loss}\\n')\n",
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(10,6))\n",
    "    plt.grid(True) #打印网格\n",
    "    #plt.gca().set_ylim(0,1)\n",
    "    plt.show()\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaler = np.load(scaler_path,allow_pickle=True).take(0)\n",
    "train_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = np.zeros(15).reshape(3,5)\n",
    "train_model = load_model(model_path)\n",
    "#load 归一化参数\n",
    "intput = test_x[0].reshape((1, windows_size, X.shape[2]))\n",
    "yhat = train_model.predict(intput, verbose=0)\n",
    "#yhat = inverse_data('TotalCount',yhat)\n",
    "#test_y_0 = inverse_data('TotalCount',test_y[0])\n",
    "yhat = np.hstack((left,yhat.reshape(3,1)))\n",
    "yhat = train_scaler.inverse_transform(yhat)\n",
    "test_y_i = train_scaler.inverse_transform(np.hstack((left,test_y[0].reshape(3,1))))\n",
    "print(\"predict:\", yhat[:,-1].reshape(1,3)[0])\n",
    "print(\"real\", test_y_i[:,-1].reshape(1,3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict = []\n",
    "real = []\n",
    "start_index = len(train_x)\n",
    "#start_index = 0\n",
    "hour_success = 0\n",
    "hour_half_success = 0\n",
    "test_len = 48*3\n",
    "for i in range(len(test_x)):\n",
    "    if(i >= test_len):\n",
    "        break\n",
    "    index = i\n",
    "    real_data = df[start_index + windows_size + index:start_index + windows_size + index +3]\n",
    "    #print(\"index:\",index,\",real:\",real_data)\n",
    "    intput = test_x[index].reshape((1, windows_size, X.shape[2]))\n",
    "    yhat = train_model.predict(intput, verbose=0)\n",
    "    yhat = np.hstack((left,yhat.reshape(3,1)))\n",
    "    yhat = train_scaler.inverse_transform(yhat)\n",
    "    yhat = yhat[:,-1].reshape(1,3)[0]\n",
    "    test_y_i = train_scaler.inverse_transform(np.hstack((left,test_y[index].reshape(3,1))))\n",
    "    test_y_i = test_y_i[:,-1].reshape(1,3)[0]\n",
    "    #yhat = inverse_data('TotalCount',yhat)\n",
    "    #test_y_0 = inverse_data('TotalCount',test_y[index])\n",
    "    for i in [1,2]:\n",
    "        if(abs((yhat[i] - test_y_i[i])/test_y_i[i]) <= 0.15):\n",
    "             if(i == 1):\n",
    "                 hour_success+=1\n",
    "             else:\n",
    "                 hour_half_success+=1\n",
    "    predict.append(yhat)\n",
    "    real.append(test_y_i)\n",
    "    print(f'{real_data.index[0]}-{real_data.index[2]}:',\"predict:\", yhat,\",real:\",test_y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test data size:{test_len}')\n",
    "print(f'hour_success:{hour_success},rate:{hour_success/test_len*100}%')\n",
    "print(f'hour_half_success:{hour_half_success},rate:{hour_half_success/test_len*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import MultipleLocator\n",
    "line = 1\n",
    "start = 0\n",
    "end =-1\n",
    "predict = np.array(predict)\n",
    "real = np.array(real)\n",
    "plt.figure(figsize=(40, 8))\n",
    "plt.plot(predict[start:end,line+1:line+2])\n",
    "plt.plot(real[start:end,line:line+1])\n",
    "plt.legend(['predict' + str(line),'real' + str(line)])\n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(MultipleLocator(48))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}